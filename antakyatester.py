# -*- coding: utf-8 -*-
"""antakyatester.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x90hDrIrL94wU1XO_kBBCc9DgaLXk-ch
"""

# STEP 1: Mount Google Drive
from google.colab import drive
import zipfile

drive.mount('/content/drive')

# STEP 2: Install dependencies
!pip install albumentations
!pip install segmentation-models-pytorch --quiet

# STEP 3: Extract tile zip to local
ZIP_PATH = "/content/drive/MyDrive/thesis/data/tiles_output.zip"
EXTRACT_PATH = "/content/tiles_local/"

with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
    zip_ref.extractall(EXTRACT_PATH)

print("‚úÖ Tiles extracted to local disk.")

# STEP 4: Imports & Config
import os
import numpy as np
import torch
import torch.nn as nn
import glob
import imageio
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import f1_score, precision_score, recall_score, jaccard_score

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BASE_DIR = "/content/tiles_local/"
CHECKPOINT_PATH = "/content/drive/MyDrive/thesis/checkpoints/checkpoints_unetpp_state_epoch15.pth"
OUT_DIR = "/content/drive/MyDrive/thesis/inference_output1"
os.makedirs(OUT_DIR, exist_ok=True)

IMG_SIZE = 256
BATCH_SIZE = 64
THRESHOLDS = [0.5, 0.4, 0.3, 0.2, 0.1]

# STEP 5: Define model
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.ReLU(inplace=True)
        )
    def forward(self, x):
        return self.block(x)

class UNetPP(nn.Module):
    def __init__(self, in_channels=6, out_channels=2):  # FIXED: use 2 channels for CE loss
        super().__init__()
        filters = [64, 128, 256, 512]
        self.conv00 = ConvBlock(in_channels, filters[0])
        self.pool0 = nn.MaxPool2d(2)
        self.conv10 = ConvBlock(filters[0], filters[1])
        self.pool1 = nn.MaxPool2d(2)
        self.conv20 = ConvBlock(filters[1], filters[2])
        self.pool2 = nn.MaxPool2d(2)
        self.conv30 = ConvBlock(filters[2], filters[3])
        self.up01 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)
        self.conv01 = ConvBlock(filters[0]*2, filters[0])
        self.up11 = nn.ConvTranspose2d(filters[2], filters[1], 2, stride=2)
        self.conv11 = ConvBlock(filters[1]*2, filters[1])
        self.up02 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)
        self.conv02 = ConvBlock(filters[0]*3, filters[0])
        self.up21 = nn.ConvTranspose2d(filters[3], filters[2], 2, stride=2)
        self.conv21 = ConvBlock(filters[2]*2, filters[2])
        self.up12 = nn.ConvTranspose2d(filters[2], filters[1], 2, stride=2)
        self.conv12 = ConvBlock(filters[1]*3, filters[1])
        self.up03 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)
        self.conv03 = ConvBlock(filters[0]*4, filters[0])
        self.final = nn.Conv2d(filters[0], out_channels, 1)

    def forward(self, x):
        x00 = self.conv00(x)
        x10 = self.conv10(self.pool0(x00))
        x20 = self.conv20(self.pool1(x10))
        x30 = self.conv30(self.pool2(x20))
        x01 = self.conv01(torch.cat([x00, self.up01(x10)], dim=1))
        x11 = self.conv11(torch.cat([x10, self.up11(x20)], dim=1))
        x02 = self.conv02(torch.cat([x00, x01, self.up02(x11)], dim=1))
        x21 = self.conv21(torch.cat([x20, self.up21(x30)], dim=1))
        x12 = self.conv12(torch.cat([x10, x11, self.up12(x21)], dim=1))
        x03 = self.conv03(torch.cat([x00, x01, x02, self.up03(x12)], dim=1))
        return self.final(x03)

# STEP 6: Load model + checkpoint
model = UNetPP(in_channels=6, out_channels=2).to(DEVICE)  # FIXED
checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# STEP 7: Load data
img_paths = sorted(glob.glob(os.path.join(BASE_DIR, "img_*.npy")))
mask_paths = sorted(glob.glob(os.path.join(BASE_DIR, "mask_*.npy")))

if len(img_paths) == 0 or len(mask_paths) == 0:
    for root, dirs, files in os.walk(BASE_DIR):
        print(f"{root}:")
        for file in files:
            print(f"  - {file}")
    raise RuntimeError("‚ùå No .npy tiles found. Check BASE_DIR or ZIP contents.")

all_imgs = [np.load(p) for p in img_paths]
all_masks = [np.load(p) for p in mask_paths]

# STEP 8: Evaluate multiple thresholds
for threshold in THRESHOLDS:
    print(f"\nüöÄ Testing Threshold: {threshold}")
    all_preds, all_truths = [], []
    tiles_with_damage, tiles_with_pred_damage = [], []

    threshold_dir = os.path.join(OUT_DIR, f"thresh_{int(threshold * 100)}")
    os.makedirs(threshold_dir, exist_ok=True)

    for i in tqdm(range(0, len(all_imgs), BATCH_SIZE)):
        batch_imgs = all_imgs[i:i + BATCH_SIZE]
        batch_masks = all_masks[i:i + BATCH_SIZE]

        imgs = np.stack(batch_imgs)
        masks = np.stack(batch_masks)

        img_tensor = torch.tensor(imgs).to(DEVICE)

        with torch.no_grad():
            logits = model(img_tensor.float())
            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # FIXED: use softmax for 2-class output
            preds = (probs > threshold).astype(np.uint8)

        for j in range(preds.shape[0]):
            idx = i + j
            pred = preds[j]
            mask = masks[j]

            if pred.shape[0] == 1:
                pred = np.squeeze(pred)
            elif pred.shape == (256, 256):
                pass
            else:
                raise ValueError(f"Unexpected prediction shape at idx {idx}: {pred.shape}")

            np.save(os.path.join(threshold_dir, f"pred_{idx:06d}.npy"), pred)
            imageio.imwrite(os.path.join(threshold_dir, f"pred_{idx:06d}.png"), pred * 255)

            all_preds.append(pred.flatten())
            all_truths.append(mask.flatten().astype(np.uint8))

            if np.any(mask):
                tiles_with_damage.append(idx)
            if np.any(pred):
                tiles_with_pred_damage.append(idx)

            if idx == 0 or idx == 13:
                print(f"üñº Visual check for tile {idx} @ threshold {threshold}")
                plt.figure(figsize=(12, 4))
                plt.subplot(1, 3, 1)
                plt.imshow(np.transpose(imgs[j][:3], (1, 2, 0)))
                plt.title("Input Image (RGB)")
                plt.subplot(1, 3, 2)
                plt.imshow(mask, cmap='gray')
                plt.title("Ground Truth Mask")
                plt.subplot(1, 3, 3)
                plt.imshow(pred, cmap='gray')
                plt.title(f"Predicted Mask @ t={threshold}")
                plt.show()

    all_preds = np.concatenate(all_preds)
    all_truths = np.concatenate(all_truths)

    print("\nüìä Threshold Results:")
    print(f"Threshold: {threshold}")
    print("Total positive pixels in GT:", np.sum(all_truths))
    print("Total positive pixels in Pred:", np.sum(all_preds))
    print("Tiles with damage:", len(tiles_with_damage))
    print("Tiles with predicted damage:", len(tiles_with_pred_damage))
    if len(tiles_with_pred_damage) == 0:
        print("‚ö†Ô∏è No predicted damage at this threshold!")

    f1 = f1_score(all_truths, all_preds, zero_division=0)
    precision = precision_score(all_truths, all_preds, zero_division=0)
    recall = recall_score(all_truths, all_preds, zero_division=0)
    iou = jaccard_score(all_truths, all_preds, zero_division=0)

    print(f"‚úÖ F1 Score:      {f1:.4f}")
    print(f"‚úÖ Precision:     {precision:.4f}")
    print(f"‚úÖ Recall:        {recall:.4f}")
    print(f"‚úÖ IoU (Jaccard): {iou:.4f}")